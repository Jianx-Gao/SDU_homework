{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 深度森林算法的简介、实现（不调用sklearn的森林模型）和实验\n",
    "\n",
    "## 3.1 深度森林的原理\n",
    "深度森林是一种新的基于决策树的集成学习算法，它通过对树构成的森林进行集成并串联起来达到让分类器做表征学习的目的，从而提高分类的效果。\n",
    "\n",
    "### 3.1.1 集成学习Bagging简介——从决策树到随机森林和极端随机森林\n",
    "\n",
    "集成学习算法本身不算一种单独的机器学习算法，而是以牺牲一部分效率为代价，通过构建并结合多个机器学习器来完成学习任务，常见的集成学习方法主要有Bagging和Boosting，下面要讲述的随机森林就是Bagging的代表算法。\n",
    "\n",
    "Bagging的算法过程为：从原始样本集中使用Bootstraping方法随机从$m$个样本中抽取$m_0$个训练样本，共进行$k$轮抽取，得到$k$个训练集。对于$k$个训练集，我们训练$k$个模型，这$k$个模型不一定是相同的，但它们的重要性相同。整个Bagging过程可以用下图中一个类似并联的形式表示：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/da896d6cca31437ebf6b89bd0808dc9111be4914e3ec43eb9fbc5af7352f01ee)\n",
    "\n",
    "\n",
    "#### 随机森林算法\n",
    "对于具体的随机森林算法，在为每棵决策树有放回地选取了一部分训练数据后，决策树的每个节点构建时，还需要从$n$个特征中随机有放回地选取$n_0$个（一般$n_0=\\sqrt{n}$），按照基尼指数对决策树进行构建。总而言之，随机森林的随机主要是两个方面，一个是随机有放回的抽取样本，一个是随机选取特征变量。\n",
    "\n",
    "#### 极端随机森林算法\n",
    "极端随机森林同样是由多棵决策树集成的分类器，它与随机森林的区别主要如下：\n",
    "\n",
    "（1）极端随机森林在特征选择时比较激进，在决策树每个结点上只随机选取一个特征来划分决策树，而不像随机森林一样基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点；\n",
    "\n",
    "（2）随机森林为每棵决策树随机选取一定样本，而极端随机森林中，有时候每棵决策树都采用原始训练集。\n",
    "\n",
    "从（1）可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于随机森林所生成的决策树。也就是说，模型的方差相对于随机森林进一步减少，但是偏差相对于随机森林进一步增大。一般情况下，\n",
    "极端随机森林分类器在分类精度和训练时间等方面都要优于随机森林分类器。\n",
    "\n",
    "- 在编写代码时，极端随机森林的实现只需要将普通的随机森林中“每次分裂选取的特征数量”调整为$1$。\n",
    "\n",
    "### 3.1.2 提出深度森林方法的动机\n",
    "深度学习方法建立在神经网络模型上，在图像、视频、音频等领域都有着强大的性能，它作为一种多层参数化的可分化非线性模块，可以通过反向传播进行训练。\n",
    "\n",
    "####然而，深度神经网络DNN也有着明显的缺点：\n",
    "\n",
    "（1）DNN有着大量需要手动设定的超参数，其学习性能严重依赖于仔细的参数调整，如此之多的干扰因素也使得DNN的理论分析也变得非常困难。\n",
    "\n",
    "（2）DNN需要大量的数据进行训练，因此DNN很难被应用于只有小规模训练数据的任务中。然而现实情况是，由于高昂的标注成本，大多实际应用场景对应的问题还是小型数据集。\n",
    "\n",
    "（3）DNN的网络结构必须在训练之前确定，这意味着模型的复杂性无法根据数据输入的情况进行自适应的调整，容易产生过度复杂的神经网络结构。\n",
    "实际上，在许多Kaggle竞赛中，随机森林和XGBoost算法的效果要显著优于DNN。\n",
    "\n",
    "####相比之下，深度森林算法：\n",
    "\n",
    "（1）需要设定的超参数少。\n",
    "\n",
    "深度森林算法中只有森林中树的类型，树的棵数等少量几个参数需要设置，具体的参数列表和DNN对比如下图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a1ad656b90aa41f493d264766ce115aeb226cec6be0741e49389d70bf9626081)\n",
    "\n",
    "\n",
    "（2）效率高，支持小规模训练数据。\n",
    "\n",
    "（3）深度森林的结构（也就是级联森林部分的深度）可以通过对数据的训练过程自适应地进行调整，不会产生过度复杂的算法结构。\n",
    "\n",
    "### 3.1.3 深度森林的两个组件——MGS和gcForest\n",
    "\n",
    "深度森林主要分为多粒度扫描和级联森林两部分。\n",
    "\n",
    "#### （1）多粒度扫描(Multi-Grain-Scanning)\n",
    "\n",
    "在常见的一些问题中，数据本身的不同特征之间，有着显著的时空间关系。例如，在图像识别问题中，位置靠近的像素点之间有很强的空间关系；一组呈现序列形式的数据也有着前后顺序上的关系。为此提出了多粒度扫描，它利用多种大小的滑动窗口进行采样，以获得不同尺度的特征子样本，这一步的处理类似于卷积的过程。\n",
    "\n",
    "多粒度扫描的一个例子如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7d1cb52c035e41b3bb4ce6602ca50c373f1e008784e24aad8e5a22a0a307b879)\n",
    "\n",
    "\n",
    "对整个MGS的过程进行举例分析如下：\n",
    "\n",
    "以一维情况的三分类问题为例，对于一个$400$维的输入特征，如果选用一个$100$维的窗口，窗口滑动的步长为$1$，则会产生$400-100+1=301$个大小为$100$维的向量。这$301$个向量在经过随机森林$A$和一个极端随机森林$B$后（每个森林输出一个分类概率的$3$维向量），把这些三维向量全部拼到一起，构成一个$301\\times 3\\times 2=1806$维的向量。至此MGS全部完成。\n",
    "\n",
    "- 需要注意的是，在编写MGS的程序时，首先要使用全部的训练数据对两个森林进行训练，训练完毕后，再将所有的特征向量输入MGS，把得到的向量再附上原有的标签，即可完成MGS过程。\n",
    "\n",
    "\n",
    "#### （2）级联森林(Cascade-Forest)\n",
    "\n",
    "级联森林的每一层都包含许多个集成学习分类器，它的本质是集成的集成。为了使生成的特征具有多样性，在每一层中采用随机森林和极端随机森林这两种不同的森林结构，这两种森林已经在3.1.1中介绍。需要强调的是，这两种森林中决策树的每个叶子结点最多能容纳一个样本。\n",
    "\n",
    "如下图，假设所有标签构成的集合的大小为$C$，在级联森林的具体结构中，输入的特征向量经过每个森林都会生成一个$C$维向量，假设每层有$F$个森林，那么每一层输出的特征向量就是$FC$维的。之后，级联森林采取了类似DenseNet的结构，将这个向量直接与原来的特征向量合并（而不是像ResNet一样相加），作为下一层的输入，直到最后一层，输出$F$个$C$维向量，把它们取平均即可得到最后的$C$维预测结果。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/62d29b0a30e74dbb9778fb71e52279de737b5e70d1d3438cad7cdb9088660108)\n",
    "\n",
    "\t\n",
    "需要注意的是，为了降低过拟合风险，每个森林生成的类向量是通过K折交叉验证产生的，也就是说，每个样本都被当作$K-1$次训练数据，产生$k-1$个$C$维向量，然后对其取平均值，生成图中的$C$维向量。\n",
    "\n",
    "每个森林具体的决策过程如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/897f849b825f4cee96edc97c4f0ef752603a4826837f452eb173c9fd8bff3208)\n",
    "\n",
    "### 3.1.4 深度森林的整体结构\n",
    "\n",
    "如下图，假设有$w$种不同大小的窗口，那么在$N$层级联森林中，每层都分别将该层中的森林输出的特征和不同大小窗口产生的输入特征向量做拼接，相当于级联森林实际上是有$w\\times N$层的。MGS和gcForest的运作均根据3.1.3中所述的流程进行。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/39876bf9c0aa431ca17f1fc6d31c7776fdbcb249d07148bd85ffe9ef8eced6b4)\n",
    "\n",
    "以上就是深度森林的整个算法流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 用Python+Paddle实现随机森林\n",
    "\n",
    "这里需要特别强调的是，在我们实现深度森林的过程中，没有调用\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraForestClassifier\n",
    "```\n",
    "这两种sklearn中提供的森林结构，所有算法的部分都是纯Python代码实现的。深度森林的两个组件以及原生Python实现的随机森林都封装在类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "首先是随机森林和极端随机森林的代码实现：\n",
    "\n",
    "(其中需要注意的是，深度森林算法中的每个森林结构都需要输出概率值，因此需要重新编写计算概率的函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import paddle\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 纯Python+Paddle实现随机森林\n",
    "\n",
    "def calculate_prob(arr,Y_label):  # 计算概率的函数\n",
    "    # 给出[5,4,5,5,5,6]，要返回[1/3,1/2,1/6]\n",
    "    prob=[]\n",
    "    dic=dict()\n",
    "    for item in arr:\n",
    "        if str(item) in dic:\n",
    "            dic[str(item)]+=1\n",
    "        else:\n",
    "            dic[str(item)]=1\n",
    "    skeys=sorted(dic.keys())\n",
    "    for item in Y_label:\n",
    "        if str(item) in dic:\n",
    "            prob.append(dic[str(item)]/len(arr))\n",
    "        else:\n",
    "            prob.append(0)\n",
    "    return prob\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    \"\"\"定义一棵决策树\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.leaf_value = None\n",
    "        self.tree_left = None\n",
    "        self.tree_right = None\n",
    "\n",
    "    def calc_predict_value(self, dataset):\n",
    "        \"\"\"通过递归决策树找到样本所属叶子节点\"\"\"\n",
    "        if self.leaf_value is not None:\n",
    "            return self.leaf_value\n",
    "        elif dataset[self.split_feature] <= self.split_value:\n",
    "            return self.tree_left.calc_predict_value(dataset)\n",
    "        else:\n",
    "            return self.tree_right.calc_predict_value(dataset)\n",
    "\n",
    "\n",
    "class RandomForestClassifier(object):\n",
    "    def __init__(self, n_estimators=10, max_depth=-1, min_samples_split=2, min_samples_leaf=1,\n",
    "                 min_split_gain=0.0, colsample_bytree=None, subsample=0.8, random_state=2021):\n",
    "        \"\"\"\n",
    "        随机森林参数\n",
    "        ----------\n",
    "        n_estimators:      树数量\n",
    "        max_depth:         树深度，-1表示不限制深度\n",
    "        min_samples_split: 节点分裂所需的最小样本数量，小于该值节点终止分裂\n",
    "        min_samples_leaf:  叶子节点最少样本数量，小于该值叶子被合并\n",
    "        min_split_gain:    分裂所需的最小增益，小于该值节点终止分裂\n",
    "        colsample_bytree:  列采样设置，可取[sqrt、log2]。sqrt表示随机选择sqrt(n_features)个特征，\n",
    "                           log2表示随机选择log(n_features)个特征，设置为其他则不进行列采样\n",
    "        subsample:         行采样比例\n",
    "        random_state:      随机种子，设置之后每次生成的n_estimators个样本集不会变，确保实验可重复\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth if max_depth != -1 else float('inf')\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_split_gain = min_split_gain\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        self.trees = None\n",
    "        self.feature_importances_ = dict()\n",
    "\n",
    "    def fit(self, dataset, targets):\n",
    "        \"\"\"模型训练入口\"\"\"\n",
    "        targets = targets.to_frame(name='label')\n",
    "\n",
    "        if self.random_state:\n",
    "            random.seed(self.random_state)\n",
    "        random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)\n",
    "\n",
    "        # 两种列采样方式\n",
    "        if self.colsample_bytree == \"sqrt\":\n",
    "            self.colsample_bytree = int(len(dataset.columns) ** 0.5)\n",
    "        elif self.colsample_bytree == \"log2\":\n",
    "            self.colsample_bytree = int(math.log(len(dataset.columns)))\n",
    "        elif self.colsample_bytree == 1:\n",
    "            self.colsample_bytree = 1\n",
    "        else:\n",
    "            self.colsample_bytree = len(dataset.columns)\n",
    "\n",
    "        # 并行建立多棵决策树\n",
    "        self.trees = Parallel(n_jobs=-1, verbose=0, backend=\"threading\")(\n",
    "            delayed(self._parallel_build_trees)(dataset, targets, random_state)\n",
    "            for random_state in random_state_stages)\n",
    "\n",
    "    def _parallel_build_trees(self, dataset, targets, random_state):\n",
    "        \"\"\"bootstrap有放回抽样生成训练样本集，建立决策树\"\"\"\n",
    "        subcol_index = random.sample(dataset.columns.tolist(), self.colsample_bytree)\n",
    "        dataset_stage = dataset.sample(n=int(self.subsample * len(dataset)), replace=True,\n",
    "                                       random_state=random_state).reset_index(drop=True)\n",
    "        dataset_stage = dataset_stage.loc[:, subcol_index]\n",
    "        targets_stage = targets.sample(n=int(self.subsample * len(dataset)), replace=True,\n",
    "                                       random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "        tree = self._build_single_tree(dataset_stage, targets_stage, depth=0)\n",
    "        # print(tree.describe_tree())\n",
    "        return tree\n",
    "\n",
    "    def _build_single_tree(self, dataset, targets, depth):\n",
    "        \"\"\"递归建立决策树\"\"\"\n",
    "        # 如果该节点的类别全都一样/样本小于分裂所需最小样本数量，则选取出现次数最多的类别。终止分裂\n",
    "        if len(targets['label'].unique()) <= 1 or dataset.__len__() <= self.min_samples_split:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "            return tree\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            best_split_feature, best_split_value, best_split_gain = self.choose_best_feature(dataset, targets)\n",
    "            left_dataset, right_dataset, left_targets, right_targets = \\\n",
    "                self.split_dataset(dataset, targets, best_split_feature, best_split_value)\n",
    "\n",
    "            tree = Tree()\n",
    "            # 如果父节点分裂后，左叶子节点/右叶子节点样本小于设置的叶子节点最小样本数量，则该父节点终止分裂\n",
    "            if left_dataset.__len__() <= self.min_samples_leaf or \\\n",
    "                    right_dataset.__len__() <= self.min_samples_leaf or \\\n",
    "                    best_split_gain <= self.min_split_gain:\n",
    "                tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "                return tree\n",
    "            else:\n",
    "                # 如果分裂的时候用到该特征，则该特征的importance加1\n",
    "                self.feature_importances_[best_split_feature] = \\\n",
    "                    self.feature_importances_.get(best_split_feature, 0) + 1\n",
    "\n",
    "                tree.split_feature = best_split_feature\n",
    "                tree.split_value = best_split_value\n",
    "                tree.tree_left = self._build_single_tree(left_dataset, left_targets, depth + 1)\n",
    "                tree.tree_right = self._build_single_tree(right_dataset, right_targets, depth + 1)\n",
    "                return tree\n",
    "        # 如果树的深度超过预设值，则终止分裂\n",
    "        else:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "            return tree\n",
    "\n",
    "    def choose_best_feature(self, dataset, targets):\n",
    "        \"\"\"寻找最好的数据集划分方式，找到最优分裂特征、分裂阈值、分裂增益\"\"\"\n",
    "        best_split_gain = 1\n",
    "        best_split_feature = None\n",
    "        best_split_value = None\n",
    "\n",
    "        for feature in dataset.columns:\n",
    "            if dataset[feature].unique().__len__() <= 100:\n",
    "                unique_values = sorted(dataset[feature].unique().tolist())\n",
    "            # 如果该维度特征取值太多，则选择100个百分位值作为待选分裂阈值\n",
    "            else:\n",
    "                unique_values = np.unique([np.percentile(dataset[feature], x)\n",
    "                                           for x in np.linspace(0, 100, 100)])\n",
    "\n",
    "            # 对可能的分裂阈值求分裂增益，选取增益最大的阈值\n",
    "            for split_value in unique_values:\n",
    "                left_targets = targets[dataset[feature] <= split_value]\n",
    "                right_targets = targets[dataset[feature] > split_value]\n",
    "                split_gain = self.calc_gini(left_targets['label'], right_targets['label'])\n",
    "\n",
    "                if split_gain < best_split_gain:\n",
    "                    best_split_feature = feature\n",
    "                    best_split_value = split_value\n",
    "                    best_split_gain = split_gain\n",
    "        return best_split_feature, best_split_value, best_split_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_leaf_value(targets):\n",
    "        \"\"\"选择样本中出现次数最多的类别作为叶子节点取值\"\"\"\n",
    "        label_counts = collections.Counter(targets)\n",
    "        major_label = max(zip(label_counts.values(), label_counts.keys()))\n",
    "        return major_label[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_gini(left_targets, right_targets):\n",
    "        \"\"\"分类树采用基尼指数作为指标来选择最优分裂点\"\"\"\n",
    "        split_gain = 0\n",
    "        for targets in [left_targets, right_targets]:\n",
    "            gini = 1\n",
    "            # 统计每个类别有多少样本，然后计算gini\n",
    "            label_counts = collections.Counter(targets)\n",
    "            for key in label_counts:\n",
    "                prob = label_counts[key] * 1.0 / len(targets)\n",
    "                gini -= prob ** 2\n",
    "            split_gain += len(targets) * 1.0 / (len(left_targets) + len(right_targets)) * gini\n",
    "        return split_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def split_dataset(dataset, targets, split_feature, split_value):\n",
    "        \"\"\"根据特征和阈值将样本划分成左右两份，左边小于等于阈值，右边大于阈值\"\"\"\n",
    "        left_dataset = dataset[dataset[split_feature] <= split_value]\n",
    "        left_targets = targets[dataset[split_feature] <= split_value]\n",
    "        right_dataset = dataset[dataset[split_feature] > split_value]\n",
    "        right_targets = targets[dataset[split_feature] > split_value]\n",
    "        return left_dataset, right_dataset, left_targets, right_targets\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"输入样本，预测所属类别\"\"\"\n",
    "        res = []\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        for _, row in dataset.iterrows():\n",
    "            pred_list = []\n",
    "            # 统计每棵树的预测结果，选取出现次数最多的结果作为最终类别\n",
    "            for tree in self.trees:\n",
    "                pred_list.append(tree.calc_predict_value(row))\n",
    "            pred_label_counts = collections.Counter(pred_list)\n",
    "            pred_label = max(zip(pred_label_counts.values(), pred_label_counts.keys()))\n",
    "            res.append(pred_label[1])\n",
    "        return np.array(res)\n",
    "\n",
    "    def predict_prob(self, dataset, Y_label):\n",
    "        res = []\n",
    "        # Y转成整数形式\n",
    "        Y=np.array(Y_label)\n",
    "        Y=Y.astype(int)\n",
    "        Y_label=Y.tolist()\n",
    "        for _, row in dataset.iterrows():\n",
    "            pred_list = []\n",
    "            # 统计每棵树的预测结果，选取出现次数最多的结果作为最终类别\n",
    "            for tree in self.trees:\n",
    "                pred_list.append(tree.calc_predict_value(row))\n",
    "            pred_list=np.array(pred_list)\n",
    "            pred_list=pred_list.astype(int)\n",
    "            pred_list=pred_list.tolist()\n",
    "            pred_prob=calculate_prob(pred_list,Y_label)\n",
    "            res.append(pred_prob)\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "之后是MGS结构的代码实现，其中包括MGS入口，区分训练MGS或执行MGS，窗口滑动的实现这三个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MGS(object):\n",
    "    '''\n",
    "    输入参数说明：\n",
    "    windows：滑动窗口的大小构成的数组\n",
    "    n_mgsTree：mgs的每个森林中树的棵数\n",
    "    min_sample_mgs：mgs每个决策树叶子结点最多能容纳的样本数\n",
    "    stride：滑动窗口的步长\n",
    "    另外，由于个人电脑内存原因，mgs中的森林数设置为2，不再设置为其余2的倍数。\n",
    "    '''\n",
    "    def __init__(self,windows,n_mgsTree,min_sample_mgs,stride):\n",
    "        self.windows=windows\n",
    "        self.n_mgsTree=n_mgsTree\n",
    "        self.min_sample_mgs=min_sample_mgs\n",
    "        self.stride=stride\n",
    "        self.prfs=[]\n",
    "        self.crfs=[]\n",
    "\n",
    "    def mg_scanning(self,X, y=None):  # MGS入口函数\n",
    "        windows=self.windows\n",
    "        self._n_samples=np.shape(X)[0] # 样本个数\n",
    "        shape_1X = len(X[0]) # 样本特征维度\n",
    "        if isinstance(shape_1X, int):\n",
    "            shape_1X = [1, shape_1X]  # 把一维特征拼成list\n",
    "        mgs_pred_prob = []\n",
    "\n",
    "        for i in range(len(self.windows)):\n",
    "            if y is None:  # 测试\n",
    "                #print(self.prfs)\n",
    "                wdw_pred_prob = self.window_slicing_pred_prob(X, self.windows[i], shape_1X, y=y,prff=self.prfs[i],crff=self.crfs[i])\n",
    "                mgs_pred_prob.append(wdw_pred_prob)\n",
    "            else:  # 训练\n",
    "                prf,crf=self.window_slicing_pred_prob(X, self.windows[i], shape_1X, y=y,prff=None,crff=None)\n",
    "                self.prfs.append(prf)\n",
    "                self.crfs.append(crf)\n",
    "        if len(mgs_pred_prob)!=0:\n",
    "            #(mgs_pred_prob)\n",
    "            return np.concatenate(mgs_pred_prob, axis=1)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def window_slicing_pred_prob(self, X, window, shape_1X, y ,prff,crff):  # 区分是训练MGS还是执行MGS\n",
    "        n_tree = self.n_mgsTree\n",
    "        min_samples = self.min_sample_mgs\n",
    "        stride = self.stride\n",
    "        sliced_X, sliced_y = self._window_slicing_sequence(X, window, shape_1X, y=y, stride=stride)\n",
    "        if y is not None:\n",
    "            print('MGS Training...')\n",
    "            prf = RandomForestClassifier(n_estimators=n_tree, colsample_bytree='sqrt',\n",
    "                                         min_samples_split=min_samples)\n",
    "            crf = RandomForestClassifier(n_estimators=n_tree, colsample_bytree=1,\n",
    "                                         min_samples_split=min_samples)\n",
    "            sliced_X=pd.DataFrame(sliced_X)\n",
    "            sliced_y=pd.Series(sliced_y)\n",
    "            prf.fit(sliced_X, sliced_y)\n",
    "            crf.fit(sliced_X, sliced_y)\n",
    "            self.all_label=list(set(sliced_y))\n",
    "            return prf,crf\n",
    "\n",
    "        else:\n",
    "            print('MGS Executing...')\n",
    "            sliced_X = pd.DataFrame(sliced_X)\n",
    "            pred_prob_prf = prff.predict_prob(sliced_X,self.all_label)\n",
    "            #print(\"*\"*100)\n",
    "            #print(pred_prob_prf)\n",
    "            pred_prob_crf = crff.predict_prob(sliced_X,self.all_label)\n",
    "            pred_prob = np.c_[pred_prob_prf, pred_prob_crf]\n",
    "            return pred_prob.reshape([self._n_samples, -1])\n",
    "\n",
    "    def _window_slicing_sequence(self, X, window, shape_1X, y=None, stride=1):  # 真正的窗口滑动函数\n",
    "        if shape_1X[1] < window:\n",
    "            raise ValueError('window must be smaller than the sequence dimension')\n",
    "\n",
    "        len_iter = np.floor_divide((shape_1X[1] - window), stride) + 1\n",
    "        iter_array = np.arange(0, stride*len_iter, stride)\n",
    "\n",
    "        ind_1X = np.arange(np.prod(shape_1X))\n",
    "        inds_to_take = [ind_1X[i:i+window] for i in iter_array]\n",
    "        sliced_sqce = np.take(X, inds_to_take, axis=1).reshape(-1, window)\n",
    "\n",
    "        if y is not None:\n",
    "            sliced_target = np.repeat(y, len_iter)\n",
    "        else:\n",
    "            sliced_target = None\n",
    "\n",
    "        return sliced_sqce, sliced_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面对iris数据集执行一次MGS过程，作为示例（由于iris本身的特征很少，窗口大小直接设为[1,3]）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGS Training...\n",
      "MGS Training...\n",
      "MGS Executing...\n",
      "MGS Executing...\n",
      "MGS之前数据维度： (105, 4)\n",
      "MGS之后数据维度： (105, 36)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 这里并没有使用sklearn中的模型\n",
    "\n",
    "MJ=MGS(windows=[1,3],n_mgsTree=2,min_sample_mgs=1,stride=1)\n",
    "df = pd.read_csv(\"iris.txt\", sep='\\t', header=None)  # 注意导入的数据集有没有列名， 没有的话就加入 header=None ，否则不加\n",
    "df = np.array(df)\n",
    "x = df[:, :-1]\n",
    "y = df[:, -1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10086)\n",
    "MJ.mg_scanning(x_train,y_train)\n",
    "Data=MJ.mg_scanning(x_train)\n",
    "print(\"MGS之前数据维度：\",x_train.shape)\n",
    "print(\"MGS之后数据维度：\",Data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "之后是级联森林部分的实现，分为级联森林的构建和级联森林的训练两个类实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义 deepforest 级联层类\n",
    "class CascadeLayer(object):\n",
    "    '''\n",
    "    参数说明：\n",
    "    num_forsts：森林个数\n",
    "    n_estimators：每个森林中树木棵数\n",
    "    max_depth：每棵树的最大深度\n",
    "    min_samples_leaf：叶子结点最小样本数\n",
    "    '''\n",
    "    def __init__(self, n_estimators, num_forests, max_depth=40, min_samples_leaf=1):  \n",
    "        self.min_samples_leaf = min_samples_leaf  \n",
    "        self.model = []  # 结果类向量\n",
    "        self.num_forests = num_forests\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y, y_col):  # 训练函数\n",
    "        proba = np.zeros([self.num_forests, y_col.shape[0]])    \n",
    "        for index in range(self.num_forests):  \n",
    "            if index % 2 == 0:  # 偶数个森林是随机森林\n",
    "                clf = RandomForestClassifier(n_estimators=5,\n",
    "                                             max_depth=5,\n",
    "                                             min_samples_split=6,\n",
    "                                             min_samples_leaf=2,\n",
    "                                             min_split_gain=0.0,\n",
    "                                             colsample_bytree=\"sqrt\",\n",
    "                                             subsample=0.8,\n",
    "                                             random_state=66)  # 这里调用原生的随机森林（sklearn中没有colsample_bytree这一属性）\n",
    "                train_data, train_label = pd.DataFrame(X), pd.Series(y)\n",
    "                clf.fit(train_data, train_label)\n",
    "                y_pd = pd.DataFrame(y_col)\n",
    "                proba[index, :] = clf.predict(y_pd)  \n",
    "            else:  # 奇数个森林是极端森林\n",
    "                clf = RandomForestClassifier(n_estimators=5,\n",
    "                                             max_depth=5,\n",
    "                                             min_samples_split=6,\n",
    "                                             min_samples_leaf=2,\n",
    "                                             min_split_gain=0.0,\n",
    "                                             colsample_bytree=1,  # 极端森林每次只选取一个特征，而不是sqrt\n",
    "                                             subsample=0.8,\n",
    "                                             random_state=66)  # 这里调用原生的极端随机森林\n",
    "                train_data, train_label = pd.DataFrame(X), pd.Series(y)\n",
    "                clf.fit(train_data, train_label)\n",
    "                y_pd = pd.DataFrame(y_col)\n",
    "                proba[index, :] = clf.predict(y_pd)  \n",
    "            self.model.append(clf)  # 创建新的级联森林层\n",
    "\n",
    "        average_proba = np.sum(proba, axis=0)  \n",
    "        average_proba /= self.num_forests  \n",
    "        val_concatenate = proba.transpose((1, 0))  \n",
    "        return average_proba, val_concatenate  # 返回平均结果和转置后的类向量矩阵\n",
    "\n",
    "    def predict(self, test_data):  # 定义预测函数\n",
    "        predict_prob = np.zeros([self.num_forests, test_data.shape[0]])\n",
    "        for forest_index, clf in enumerate(self.model):\n",
    "            predict_prob[forest_index, :] = clf.predict(test_data)\n",
    "\n",
    "        predict_avg = np.sum(predict_prob, axis=0)\n",
    "        predict_avg /= self.num_forests\n",
    "        predict_concatenate = predict_prob.transpose((1, 0))\n",
    "        return predict_avg, predict_concatenate\n",
    "\n",
    "\n",
    "# K折交叉检验训练级联森林\n",
    "class CascadeTrain(object):\n",
    "    '''\n",
    "    参数：\n",
    "    num_forests：每层的森林数量\n",
    "    n_estimators：森林中树的数量\n",
    "    n_fold：k折的k\n",
    "    kf：交叉验证的下标\n",
    "    layer_index：层索引\n",
    "    max_depth：树的最大深度\n",
    "    min_samples_leaf：叶子结点最小样本数\n",
    "    '''\n",
    "    def __init__(self, num_forests, n_estimators, n_fold, kf, layer_index, max_depth=31, min_samples_leaf=1):\n",
    "        self.num_forests = num_forests\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_fold = n_fold\n",
    "        self.kf = kf\n",
    "        self.layer_index = layer_index\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.model = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        proba = np.empty([num_samples])\n",
    "        matrix1 = np.empty([num_samples, self.num_forests])\n",
    "        for train_index, test_index in self.kf:  \n",
    "            X_train = X[train_index, :]\n",
    "            X_val = X[test_index, :]\n",
    "            y_train = y[train_index]\n",
    "            gclayer = CascadeLayer(self.n_estimators, self.num_forests, self.max_depth, self.min_samples_leaf)\n",
    "            proba[test_index], matrix1[test_index, :] = gclayer.fit(X_train, y_train, X_val)\n",
    "            self.model.append(gclayer)  \n",
    "        return [proba, matrix1]\n",
    "\n",
    "    def predict(self, X):  # 定义预测函数，用做下一层的训练数据\n",
    "        pred_prob = np.zeros([X.shape[0]])\n",
    "        for layer in self.model:\n",
    "            temp_prob, temp_prob_concatenate = layer.predict(X)\n",
    "            pred_prob += temp_prob\n",
    "        pred_prob /= self.n_fold\n",
    "        return pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "最后是整个深度森林对应的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义deepforest类\n",
    "class gcForest(object):\n",
    "    def __init__(self, num_estimator, num_forests, max_layer=100, max_depth=30, n_fold=6):\n",
    "        # 分别是森林个数，森林中树的个数，森林最大层数，最大深度，交叉验证折数\n",
    "        self.num_estimator = num_estimator\n",
    "        self.num_forests = num_forests\n",
    "        self.n_fold = n_fold\n",
    "        self.max_depth = max_depth\n",
    "        self.max_layer = max_layer\n",
    "        self.model = []     # 保存森林中的model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        train_loss = 0.0\n",
    "        layer = 0\n",
    "        top_layer = 0\n",
    "        judge_layer = 0\n",
    "\n",
    "        kf_flod = KFold(self.n_fold).split(X)\n",
    "        print('================================')\n",
    "        print('Model starts...')\n",
    "        print('--------------------------------')\n",
    "        # 开始验证\n",
    "        while layer < self.max_layer:\n",
    "            # 初始化森林新的layer\n",
    "            new_layer = CascadeTrain(self.num_forests, self.num_estimator, self.n_fold,\n",
    "                                     kf_flod, layer, self.max_depth, 1)\n",
    "            # 当前层进行训练\n",
    "            proba, gcf_stack = new_layer.fit(X, y)\n",
    "            # 计算当前层的预测loss\n",
    "            temp_val_loss = self.eval(y, proba)\n",
    "            print(\"layer : {} | Val Loss : {:.6f}\".format(layer, temp_val_loss))\n",
    "            # loss下降小于阈值，停止增长\n",
    "            if train_loss < temp_val_loss:\n",
    "                judge_layer += 1\n",
    "            else:\n",
    "                judge_layer = 0\n",
    "                train_loss = temp_val_loss\n",
    "                top_layer = layer\n",
    "            if judge_layer >= 3:\n",
    "                break\n",
    "            # 开始下一层\n",
    "            layer = layer + 1\n",
    "            self.model.append(new_layer)\n",
    "        print('================================')\n",
    "\n",
    "        for index in range(len(self.model), top_layer + 1, -1):  # 删除多余的层\n",
    "            self.model.pop()\n",
    "\n",
    "    def predict(self, X):\n",
    "        for layer in self.model:\n",
    "            prediction = layer.predict(X)\n",
    "        return prediction\n",
    "\n",
    "    def eval(self, real, predict):  # 计算预测准确率\n",
    "        temp = np.log(abs(real + 1)) - np.log(abs(predict + 1))\n",
    "        res = np.dot(temp, temp) / len(temp)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "最后是整个深度森林模型的测试：（注意，全程没有调用sklearn.ensemble）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGS Training...\n",
      "MGS Training...\n",
      "MGS Executing...\n",
      "MGS Executing...\n",
      "MGS Training...\n",
      "MGS Training...\n",
      "MGS Executing...\n",
      "MGS Executing...\n",
      "MGS执行时间： 31.151531219482422\n",
      "(105, 4)\n",
      "(105, 36)\n",
      "(45, 36)\n",
      "================================\n",
      "Model starts...\n",
      "--------------------------------\n",
      "layer : 0 | Val Loss : 0.057244\n",
      "layer : 1 | Val Loss : 0.752585\n",
      "================================\n",
      "\n",
      "training time: 11.512 s\n",
      "prediction time: 0.088 s\n",
      "\n",
      "带MGS的深度森林在iris上的准确率: 91.111 %\n",
      "================================\n",
      "Model starts...\n",
      "--------------------------------\n",
      "layer : 0 | Val Loss : 0.006121\n",
      "layer : 1 | Val Loss : 0.621897\n",
      "================================\n",
      "\n",
      "training time: 10.631 s\n",
      "prediction time: 0.079 s\n",
      "\n",
      "不带MGS的深度森林在iris上的准确率: 93.333 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"iris.txt\", sep='\\t', header=None)  # 注意导入的数据集有没有列名， 没有的话就加入 header=None ，否则不加\n",
    "    df = np.array(df)\n",
    "    x = df[:, :-1]\n",
    "    y = df[:, -1]\n",
    "    # MGS训练+执行\n",
    "    start0=time()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2021)\n",
    "    MJ = MGS(windows=[1, 3], n_mgsTree=10, min_sample_mgs=1, stride=1)\n",
    "    MJ.mg_scanning(x_train, y_train)\n",
    "    Data_train = MJ.mg_scanning(x_train)\n",
    "    MJ.mg_scanning(x_test, y_test)\n",
    "    Data_test = MJ.mg_scanning(x_test)\n",
    "    end0=time()\n",
    "    print('MGS执行时间：',end0-start0)\n",
    "    print(x_train.shape)\n",
    "    print(Data_train.shape)\n",
    "    print(Data_test.shape)\n",
    "    clf = gcForest(num_estimator=100, num_forests=4, max_layer=2, max_depth=100, n_fold=5)\n",
    "    # 训练\n",
    "    start = time()\n",
    "    clf.fit(Data_train, y_train)\n",
    "    end = time()\n",
    "    print(\"\\ntraining time: {:.3f} s\".format(end - start))\n",
    "    # 预测\n",
    "    start = time()\n",
    "    pred_y = clf.predict(Data_test).astype('int')  # 预测标签转应为整数类型\n",
    "    end = time()\n",
    "    print(\"prediction time: {:.3f} s\".format(end - start))\n",
    "    acc = accuracy_score(y_test, pred_y) * 100\n",
    "    print(\"\\n带MGS的深度森林在iris上的准确率: {:.3f} %\".format(acc))\n",
    "\n",
    "    clf = gcForest(num_estimator=100, num_forests=4, max_layer=2, max_depth=100, n_fold=5)\n",
    "    start = time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    end = time()\n",
    "    print(\"\\ntraining time: {:.3f} s\".format(end - start))\n",
    "    # 预测\n",
    "    start = time()\n",
    "    pred_y = clf.predict(x_test).astype('int')  # 预测标签转应为整数类型\n",
    "    end = time()\n",
    "    print(\"prediction time: {:.3f} s\".format(end - start))\n",
    "    acc = accuracy_score(y_test, pred_y) * 100\n",
    "    print(\"\\n不带MGS的深度森林在iris上的准确率: {:.3f} %\".format(acc))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "值得注意的是，MGS层在数据集的特征很少（如iris的4个特征）时，有可能会起到一定的消极作用，因为窗口可能会变得过小，甚至发生1个特征输入MGS的两种森林的情况。由此可见，在数据集很小时，可以考虑不使用MGS层，只使用后面的gcForest层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 深度森林算法在百余个测试数据集上的结果展示\n",
    "\n",
    "在文件final_result3.3中，我们给出了在160个测试数据集上的结果。这些结果是实现了问题四中的几种交叉检验方法（这几种方法的实现参见下文）后一起运行深度森林和随机森林方法得到的。\n",
    "\n",
    "\n",
    "其中每一列的意义如下所示：（这个结果同样用于第四问）\n",
    "\n",
    "- CV_Procedure是选用的交叉检验方法；\n",
    "\n",
    "- n_rows是数据集的行数，也就是样本个数；\n",
    "\n",
    "- n_cols是数据集的列数，也就是特征个数；\n",
    "\n",
    "- EPE_DF是K折交叉检验后深度森林方法的准确率的均值；\n",
    "\n",
    "- VAR_DF是K折交叉检验后深度森林方法的准确率的方差；\n",
    "\n",
    "- Time_DF是深度森林方法运行用时；\n",
    "\n",
    "- EPE_RF是K折交叉检验后随机森林方法的准确率的均值；\n",
    "\n",
    "- VAR_RF是K折交叉检验后随机森林方法的准确率的方差；\n",
    "\n",
    "- Time_RF是随机森林方法运行用时。\n",
    "\n",
    "深度森林方法、随机森林方法的准确率对比请重点关注EPE_DF和EPE_RF两列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4 探究对比深度森林与随机森林\n",
    "本检验方案旨在通过不同的交叉验证算法，尽可能消除抽样对算法性能的影响，分别使用**BDS交叉验证算法**、**蒙特卡洛交叉验证算法**、**分层交叉验证算法**、**留一交叉验证算法**测试深度森林算法和随机森林算法的准确率、稳定性与速度，从而得到哪种算法更为优异。\n",
    "## 4.1 BDS-Cross-Validation\n",
    "\n",
    "我们使用BDS交叉验证算法，测试算法准确率、稳定性与速度\n",
    "\n",
    "我们这里直接给出伪代码：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cedf56ed0c924782a46889b8710f59cbcf2c522036a64694971eb7a904709d7c)\n",
    "\n",
    "大家如果对该交叉验证具体数学证明感兴趣，可以读山东大学数据科学研究院副院长郭亮教授的paper：\n",
    "\n",
    "[Subsampling Bias and The Best-Discrepancy Systematic Cross Validation](http://arxiv.org/abs/1907.02437)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从.txt文件中读取数据\r\n",
    "def read_data(path):\r\n",
    "    dataset = []\r\n",
    "    with open(path) as f:\r\n",
    "        for row in f:\r\n",
    "            if \"\\t\\t\"in row:\r\n",
    "                return np.array(dataset)\r\n",
    "            row = row.replace(\"\\t\\n\",\"\")\r\n",
    "            row = row.replace(\"\\n\",\"\")\r\n",
    "            rows = row.split(\"\\t\")\r\n",
    "            if len(rows)==1:\r\n",
    "                continue\r\n",
    "            dataset.append(np.array([float(i) for i in rows]))\r\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import time\r\n",
    "\r\n",
    "\r\n",
    "# 生成BDS序列\r\n",
    "def generate_BDS(n):\r\n",
    "    temp = np.array([np.array([i*np.e - int(i*np.e)]) for i in range(1, n + 1)])\r\n",
    "    rank = np.array([np.array([i]) for i in range(n)])\r\n",
    "    BDS = np.hstack((temp, rank))\r\n",
    "    return BDS[np.argsort(BDS[:, 0]), -1].astype(np.int)\r\n",
    "\r\n",
    "\r\n",
    "# 选择降维方法，默认使用PCA方法进行降维\r\n",
    "def reduce_dimension(Data, method=\"PCA\"):\r\n",
    "    from sklearn.decomposition import PCA, FactorAnalysis, KernelPCA, TruncatedSVD\r\n",
    "    if method == 'PCA':\r\n",
    "        reduce_model = PCA(n_components=1)\r\n",
    "    elif method == 'FactorAnalysis':\r\n",
    "        reduce_model = FactorAnalysis(n_components=1)\r\n",
    "    elif method == 'KernelPCA':\r\n",
    "        reduce_model = KernelPCA(kernel='rbf', n_components=1)\r\n",
    "    elif method == 'TruncatedSVD':\r\n",
    "        reduce_model = TruncatedSVD(1)\r\n",
    "    return reduce_model.fit_transform(Data)\r\n",
    "\r\n",
    "\r\n",
    "# 单个降维方法进行BDSCV操作\r\n",
    "def BDSCV_sub(D, reduce_method,model,k):\r\n",
    "    # Todo: 通过降维算法把D降至1维\r\n",
    "    E = reduce_dimension(D, reduce_method)\r\n",
    "    # Todo: 把E添加到D的最后一列\r\n",
    "    new_D = np.hstack((D,E))\r\n",
    "    # Todo: 根据最后一列升序排序，删除最后一列\r\n",
    "    sort_D = new_D[np.argsort(new_D[:, -1]), :-1]+1\r\n",
    "    R = generate_BDS(len(sort_D))\r\n",
    "    from sklearn.model_selection import KFold\r\n",
    "    k_Cross = KFold(n_splits=k)\r\n",
    "    score_list = np.array([])\r\n",
    "    for train, test in k_Cross.split(R):\r\n",
    "        train_index, test_index = R[train], R[test]\r\n",
    "        train_data, train_label = sort_D[train_index, :-1], sort_D[train_index, -1]\r\n",
    "        test_data, test_label = sort_D[test_index, :-1], sort_D[test_index, -1]\r\n",
    "        if model==\"df\":\r\n",
    "            model = CascadeForestClassifier(n_trees=500,verbose=0,random_state=1)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        else:\r\n",
    "            model = RandomForestClassifier(n_estimators=500)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        from sklearn.metrics import accuracy_score\r\n",
    "        score = accuracy_score(pred, test_label)\r\n",
    "        score_list = np.append(score_list, score)\r\n",
    "    return score_list\r\n",
    "\r\n",
    "\r\n",
    "# BDSCV测试模型的性能\r\n",
    "def BDS_Cross_Validation(D, model, k):\r\n",
    "    start_start = time.time()\r\n",
    "    EPEs = []\r\n",
    "    VARs = []\r\n",
    "    score_pca = BDSCV_sub(D, \"PCA\", model, k)\r\n",
    "    score_fa = BDSCV_sub(D, \"FactorAnalysis\", model,k)\r\n",
    "    score_kpca = BDSCV_sub(D, 'KernelPCA', model,k)\r\n",
    "    score_tsvd = BDSCV_sub(D, 'TruncatedSVD', model,k)\r\n",
    "    EPEs.append(np.mean(score_pca))\r\n",
    "    EPEs.append(np.mean(score_fa))\r\n",
    "    EPEs.append(np.mean(score_kpca))\r\n",
    "    EPEs.append(np.mean(score_tsvd))\r\n",
    "    VARs.append(np.var(score_pca))\r\n",
    "    VARs.append(np.var(score_fa))\r\n",
    "    VARs.append(np.var(score_kpca))\r\n",
    "    VARs.append(np.var(score_tsvd))\r\n",
    "    EPEs,VARs = np.array(EPEs),np.array(VARs)\r\n",
    "    index = np.argmin(EPEs)\r\n",
    "    end_start = time.time()\r\n",
    "    return EPEs[index], VARs[index], end_start-start_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  4.2 Monte_Carlo_Cross_Validation(MCCV)\n",
    "蒙特卡洛交叉验证算法，测试算法准确率、稳定性与速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "\r\n",
    "# 蒙特卡洛交叉验证（默认重复50遍）\r\n",
    "def Monte_Carlo_Cross_Validation(dataset,model,k,repeat=50):\r\n",
    "    from sklearn.model_selection import KFold as MCCV\r\n",
    "    from sklearn.metrics import accuracy_score\r\n",
    "    start_time = time.time()\r\n",
    "    EPEs = np.array([])\r\n",
    "    VARs = np.array([])\r\n",
    "    for i in range(repeat):\r\n",
    "        k_Cross = MCCV(n_splits=k, random_state=1, shuffle=True)\r\n",
    "        score_list = np.array([])\r\n",
    "        data,label = dataset[:,:-1],dataset[:,-1]\r\n",
    "        for train_index, test_index in k_Cross.split(dataset):\r\n",
    "            train_data, train_label = dataset[train_index, :-1], dataset[train_index, -1]\r\n",
    "            test_data, test_label = dataset[test_index, :-1], dataset[test_index, -1]\r\n",
    "            if model==\"df\":\r\n",
    "                model = CascadeForestClassifier(n_trees=500,verbose=0,random_state=1)\r\n",
    "                model.fit(train_data, train_label)\r\n",
    "                pred = model.predict(test_data)\r\n",
    "            else:\r\n",
    "                model = RandomForestClassifier(n_estimators=500)\r\n",
    "                model.fit(train_data, train_label)\r\n",
    "                pred = model.predict(test_data)\r\n",
    "            score = accuracy_score(pred, test_label)\r\n",
    "            score_list = np.append(score_list, score)\r\n",
    "        EPEs = np.append(EPEs,np.mean(score_list))\r\n",
    "        VARs = np.append(VARs,np.var(score_list))\r\n",
    "    end_time = time.time()\r\n",
    "    return np.mean(EPEs),np.mean(VARs),end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.3 Stratified_MCCV\n",
    "分层交叉验证算法，测试算法准确率、稳定性与速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 分层交叉验证\r\n",
    "def Monte_Carlo_Cross_Validation_Stratified(dataset,model,k):\r\n",
    "    from sklearn.model_selection import StratifiedKFold as Stratified_MCCV\r\n",
    "    from sklearn.metrics import accuracy_score\r\n",
    "    start_time = time.time()\r\n",
    "    k_Cross = Stratified_MCCV(n_splits=k, random_state=1, shuffle=True)\r\n",
    "    score_list = np.array([])\r\n",
    "    data,label = dataset[:,:-1],dataset[:,-1]\r\n",
    "    for train_index, test_index in k_Cross.split(data,label):\r\n",
    "        train_data, train_label = data[train_index, :], label[train_index]\r\n",
    "        test_data, test_label = data[test_index, :], label[test_index]\r\n",
    "        if model==\"df\":\r\n",
    "            model = CascadeForestClassifier(n_trees=500,verbose=0,random_state=1)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        else:\r\n",
    "            model = RandomForestClassifier(n_estimators=500)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        score = accuracy_score(pred, test_label)\r\n",
    "        score_list = np.append(score_list, score)\r\n",
    "    end_time = time.time()\r\n",
    "    return np.mean(score_list),np.var(score_list),end_time-start_time\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.4 LeaveOneOut(LOO)\n",
    "留一交叉验证算法，测试算法准确率、稳定性与速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 留一交叉验证\r\n",
    "def LeaveOneOut(dataset,model):\r\n",
    "    from sklearn.model_selection import LeaveOneOut as LOO\r\n",
    "    from sklearn.metrics import accuracy_score\r\n",
    "    start_time = time.time()\r\n",
    "    loo = LOO()\r\n",
    "    score_list = np.array([])\r\n",
    "    for train_index, test_index in loo.split(dataset):\r\n",
    "        train_data, train_label = dataset[train_index, :-1], dataset[train_index, -1]\r\n",
    "        test_data, test_label = dataset[test_index, :-1], dataset[test_index, -1]\r\n",
    "        if model==\"df\":\r\n",
    "            model = CascadeForestClassifier(n_trees=500,verbose=0,random_state=1)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        else:\r\n",
    "            model = RandomForestClassifier(n_estimators=500)\r\n",
    "            model.fit(train_data, train_label)\r\n",
    "            pred = model.predict(test_data)\r\n",
    "        score = accuracy_score(pred, test_label)\r\n",
    "        score_list = np.append(score_list, score)\r\n",
    "    end_time = time.time()\r\n",
    "    return np.mean(score_list),np.var(score_list),end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.5 针对单个算法测试单个txt范例\n",
    "分别使用**BDS交叉验证算法**、**蒙特卡洛交叉验证算法**、**分层交叉验证算法**、**留一交叉验证算法**测试随机森林算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------BDS_Cross_Validation---------------------\n",
      "EPE:\t0.7830882352941176\n",
      "VAR:\t0.009515570934256052\n",
      "Time :\t10.008927822113037s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "EPE:\t0.785294117647059\n",
      "VAR:\t0.008772707612456744\n",
      "Time :\t114.31486201286316s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "EPE:\t0.7720588235294118\n",
      "VAR:\t0.003114186851211072\n",
      "Time :\t2.2797107696533203s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "EPE:\t0.7710843373493976\n",
      "VAR:\t0.17651328204383798\n",
      "Time :\t38.19196939468384s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def result_out(m,v,t):\r\n",
    "    print(\"Mean:\\t\",m)\r\n",
    "    print(\"Var:\\t\",v)\r\n",
    "    print(\"Time:\\t\",t)\r\n",
    "\r\n",
    "D = read_data(\"randomforest_datasets/analcatdata_asbestos.txt\")\r\n",
    "m,v,t = BDS_Cross_Validation(D,'rf', 5)\r\n",
    "print(\"---------------------BDS_Cross_Validation---------------------\")\r\n",
    "result_out(m,v,t)\r\n",
    "\r\n",
    "m,v,t = Monte_Carlo_Cross_Validation(D,'rf',5,repeat=50)\r\n",
    "print(\"---------------------Monte_Carlo_Cross_Validation---------------------\")\r\n",
    "result_out(m,v,t)\r\n",
    "\r\n",
    "m,v,t = Monte_Carlo_Cross_Validation_Stratified(D,'rf',5)\r\n",
    "print(\"---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\")\r\n",
    "result_out(m,v,t)\r\n",
    "\r\n",
    "m,v,t = LeaveOneOut(D,'rf')\r\n",
    "print(\"---------------------LeaveOneOut---------------------\")\r\n",
    "result_out(m,v,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.6 测试对比深度森林和随机森林的表现\n",
    "\n",
    "以[160个公开数据集](http://aistudio.baidu.com/aistudio/datasetdetail/89765)为例，展示如何对比深度森林和随机森林算法性能。鉴于测试所有数据所需时间过长，这里给出测试5个数据集\n",
    "\n",
    "\"auto.txt\",\"analcatdata_asbestos.txt\",\"analcatdata_boxing1.txt\",\"iris.txt\",\"postoperative-patient-data.txt\"\n",
    "\n",
    "的测试结果，以及测试所有数据集的final_result.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with file: auto.txt\n",
      "---------------------BDS_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.8023170731707318\n",
      "VAR:\t0.00300898274836407\n",
      "Time :\t30.732688188552856s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.797560975609756\n",
      "VAR:\t0.0020623140987507424\n",
      "Time :\t11.315667390823364s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7810365853658536\n",
      "VAR:\t0.005364910767400355\n",
      "Time :\t33.58285117149353s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7775121951219512\n",
      "VAR:\t0.005540523497917904\n",
      "Time :\t28.25880265235901s\n",
      "\n",
      "Monte_Carlo_Cross_Validation_Stratified...\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7729268292682927\n",
      "VAR:\t0.004191046995835812\n",
      "Time :\t7.773919105529785s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7729268292682927\n",
      "VAR:\t0.004904907792980368\n",
      "Time :\t2.811765193939209s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.8267326732673267\n",
      "VAR:\t0.14324576021958632\n",
      "Time :\t123.64608836174011s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.8267326732673267\n",
      "VAR:\t0.14324576021958632\n",
      "Time :\t119.4360773563385s\n",
      "\n",
      "Deal with file: analcatdata_asbestos.txt\n",
      "---------------------BDS_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7602941176470588\n",
      "VAR:\t0.01544874567474048\n",
      "Time :\t35.3968026638031s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7830882352941176\n",
      "VAR:\t0.009515570934256052\n",
      "Time :\t9.231960535049438s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7841176470588235\n",
      "VAR:\t0.009153330449826988\n",
      "Time :\t28.414267539978027s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7852941176470588\n",
      "VAR:\t0.008772707612456746\n",
      "Time :\t22.52329421043396s\n",
      "\n",
      "Monte_Carlo_Cross_Validation_Stratified...\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7602941176470589\n",
      "VAR:\t0.0038408304498269872\n",
      "Time :\t7.911222457885742s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7720588235294118\n",
      "VAR:\t0.003114186851211072\n",
      "Time :\t2.3315579891204834s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7831325301204819\n",
      "VAR:\t0.16983597038757442\n",
      "Time :\t43.28623557090759s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7710843373493976\n",
      "VAR:\t0.17651328204383798\n",
      "Time :\t38.82721304893494s\n",
      "\n",
      "Deal with file: analcatdata_boxing1.txt\n",
      "---------------------BDS_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7\n",
      "VAR:\t0.007916666666666666\n",
      "Time :\t36.33796668052673s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7\n",
      "VAR:\t0.006527777777777776\n",
      "Time :\t9.474384546279907s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7058333333333333\n",
      "VAR:\t0.001361111111111112\n",
      "Time :\t27.376921892166138s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7133333333333332\n",
      "VAR:\t0.002347222222222222\n",
      "Time :\t23.6271915435791s\n",
      "\n",
      "Monte_Carlo_Cross_Validation_Stratified...\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.75\n",
      "VAR:\t0.007638888888888887\n",
      "Time :\t10.35964560508728s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7833333333333333\n",
      "VAR:\t0.001666666666666666\n",
      "Time :\t2.4381260871887207s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.7\n",
      "VAR:\t0.20999999999999996\n",
      "Time :\t61.34266972541809s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.7083333333333334\n",
      "VAR:\t0.20659722222222227\n",
      "Time :\t57.170604944229126s\n",
      "\n",
      "Deal with file: iris.txt\n",
      "---------------------BDS_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.9400000000000001\n",
      "VAR:\t0.000622222222222222\n",
      "Time :\t25.527014017105103s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.9400000000000001\n",
      "VAR:\t0.0015111111111111102\n",
      "Time :\t9.273473501205444s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.9540000000000001\n",
      "VAR:\t0.001146666666666666\n",
      "Time :\t27.278985023498535s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.9533333333333334\n",
      "VAR:\t0.001155555555555555\n",
      "Time :\t22.98251700401306s\n",
      "\n",
      "Monte_Carlo_Cross_Validation_Stratified...\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.9466666666666667\n",
      "VAR:\t0.0002666666666666665\n",
      "Time :\t9.723721504211426s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.9666666666666666\n",
      "VAR:\t1.232595164407831e-32\n",
      "Time :\t2.2882368564605713s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.9533333333333334\n",
      "VAR:\t0.04448888888888889\n",
      "Time :\t75.32919716835022s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.9533333333333334\n",
      "VAR:\t0.04448888888888889\n",
      "Time :\t71.56522345542908s\n",
      "\n",
      "Deal with file: postoperative-patient-data.txt\n",
      "---------------------BDS_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.6477124183006536\n",
      "VAR:\t0.015337690631808277\n",
      "Time :\t41.828121185302734s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.6241830065359476\n",
      "VAR:\t0.0026570976974667846\n",
      "Time :\t9.477946281433105s\n",
      "\n",
      "---------------------Monte_Carlo_Cross_Validation---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.6619607843137254\n",
      "VAR:\t0.006484258191293946\n",
      "Time :\t28.91695261001587s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.6630718954248366\n",
      "VAR:\t0.0063070613866461605\n",
      "Time :\t23.32699680328369s\n",
      "\n",
      "Monte_Carlo_Cross_Validation_Stratified...\n",
      "---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.6366013071895424\n",
      "VAR:\t0.00048442906574394347\n",
      "Time :\t10.541332244873047s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.6254901960784314\n",
      "VAR:\t0.0015447050279806902\n",
      "Time :\t2.513442277908325s\n",
      "\n",
      "---------------------LeaveOneOut---------------------\n",
      "Deep Forest:\n",
      "EPE:\t0.6590909090909091\n",
      "VAR:\t0.22469008264462811\n",
      "Time :\t47.45338726043701s\n",
      "\n",
      "Random Forest:\n",
      "EPE:\t0.6590909090909091\n",
      "VAR:\t0.22469008264462811\n",
      "Time :\t41.64254689216614s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "\r\n",
    "# 将结果写入文件file_path\r\n",
    "def write_result(data,file_path):\r\n",
    "    with open(file_path,\"a\") as f:\r\n",
    "        for e in data:\r\n",
    "            f.write(str(e))\r\n",
    "            f.write(\",\")\r\n",
    "        f.write(\"\\n\")\r\n",
    "\r\n",
    "# 获取所有txt文件\r\n",
    "def get_file_path(dir):\r\n",
    "    file_dir_list = []\r\n",
    "    for root, dirs, files in os.walk(dir, topdown=False):\r\n",
    "        for name in files:\r\n",
    "            file_dir_list.append(os.path.join(root, name))\r\n",
    "    return file_dir_list, files\r\n",
    "file_path = \"result.csv\"\r\n",
    "write_result(['Index','File_name','CV_Procedure','n_rows','n_cols','EPE_DF','VAR_DF','Time_DF','EPE_RF','VAR_RF','Time_RF'],file_path)\r\n",
    "\r\n",
    "file_names = [\"auto.txt\",\"analcatdata_asbestos.txt\",\"analcatdata_boxing1.txt\",\"iris.txt\",\"postoperative-patient-data.txt\"]\r\n",
    "for i in range(5):\r\n",
    "    file_name = file_names[i]\r\n",
    "    path = os.path.join(\"randomforest_datasets\",file_name)\r\n",
    "    D = read_data(path)\r\n",
    "    print(\"Deal with file:\",file_name)\r\n",
    "\r\n",
    "    print(\"---------------------BDS_Cross_Validation---------------------\")\r\n",
    "    m_df, v_df, t_df = BDS_Cross_Validation(D, \"df\", 5)\r\n",
    "    print(\"Deep Forest:\")\r\n",
    "    result_out(m_df, v_df, t_df)\r\n",
    "    m_rf, v_rf, t_rf = BDS_Cross_Validation(D, \"rf\", 5)\r\n",
    "    print(\"Random Forest:\")\r\n",
    "    result_out(m_rf, v_rf, t_rf)\r\n",
    "    write_result([i,file_name,'BDSCV',len(D),len(D[0]),m_df, v_df, t_df, m_rf, v_rf, t_rf],file_path)\r\n",
    "\r\n",
    "    print(\"---------------------Monte_Carlo_Cross_Validation---------------------\")\r\n",
    "    m_df, v_df, t_df = Monte_Carlo_Cross_Validation(D, \"df\", 5, repeat=10)\r\n",
    "    print(\"Deep Forest:\")\r\n",
    "    result_out(m_df, v_df, t_df)\r\n",
    "    m_rf, v_rf, t_rf = Monte_Carlo_Cross_Validation(D, \"rf\", 5, repeat=10)\r\n",
    "    print(\"Random Forest:\")\r\n",
    "    result_out( m_rf, v_rf, t_rf)\r\n",
    "    write_result([i,file_name,'MCCV',len(D),len(D[0]),m_df, v_df, t_df, m_rf, v_rf, t_rf],file_path)\r\n",
    "    print(\"Monte_Carlo_Cross_Validation_Stratified...\")\r\n",
    "\r\n",
    "    print(\"---------------------Monte_Carlo_Cross_Validation_Stratified---------------------\")\r\n",
    "    m_df, v_df, t_df = Monte_Carlo_Cross_Validation_Stratified(D, \"df\", 5)\r\n",
    "    print(\"Deep Forest:\")\r\n",
    "    result_out(m_df, v_df, t_df)\r\n",
    "    m_rf, v_rf, t_rf = Monte_Carlo_Cross_Validation_Stratified(D, \"rf\", 5)\r\n",
    "    print(\"Random Forest:\")\r\n",
    "    result_out(m_rf, v_rf, t_rf)\r\n",
    "    write_result([i,file_name,'Stratified_MCCV',len(D),len(D[0]),m_df, v_df, t_df, m_rf, v_rf, t_rf],file_path)\r\n",
    "\r\n",
    "    print(\"---------------------LeaveOneOut---------------------\")\r\n",
    "    m_df, v_df, t_df = LeaveOneOut(D, \"df\")\r\n",
    "    print(\"Deep Forest:\")\r\n",
    "    result_out(m_df, v_df, t_df)\r\n",
    "    m_rf, v_rf, t_rf = LeaveOneOut(D, \"rf\")\r\n",
    "    print(\"Random Forest:\")\r\n",
    "    result_out(m_rf, v_rf, t_rf)\r\n",
    "    write_result([i,file_name,'LOO',len(D),len(D[0]),m_df, v_df, t_df, m_rf, v_rf, t_rf],file_path)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.7 结论\n",
    "这边再次提醒读者注意查看final_result3.3.xlsx，所有的结果均存储在这个表格中。\n",
    "\n",
    "我们直接给出结论，总体来说，在准确率和稳定上深度森林是**优于**随机森林的，在运行速度会上深度森林**劣于**随机森林。\n",
    "\n",
    "因此，**深度森林是优于随机森林的。**\n",
    "\n",
    "我们打开final_result.xlsx：\n",
    "\n",
    "- 针对n_cols升序排序，我们可以得知，当数据集特征较少时，深度森林**劣于**随机森林。\n",
    "- 针对n_cols和n_rows升序排序，我们可以得知，当数据集较小时，深度森林**劣于**随机森林。\n",
    "- 针对n_cols降序排序，我们可以得知，当数据集特征较多时，深度森林**优于**随机森林。\n",
    "- 针对n_cols和n_rows降序排序，我们可以得知，当数据集较大时，深度森林**优于**随机森林。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
